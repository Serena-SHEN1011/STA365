{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ed88fea",
   "metadata": {},
   "source": [
    "## Week 6 Homework\n",
    "\n",
    "---\n",
    "\n",
    "### Q1: Let's start having some *real* fun...\n",
    "\n",
    "We previously considered the normal-gamma specification \n",
    "\n",
    "$$\\scriptsize\n",
    "\\begin{align*}\n",
    "p(\\theta,\\tau|x) &\\propto{} p(\\theta,\\tau,x) = p(x|\\theta)p(\\theta)p(\\tau) \\quad (\\theta \\perp\\!\\!\\perp \\tau) \\leftarrow \\text{independent priors} & p(\\theta|x,\\theta_0,\\tau_0, \\tau) &={} \\text{N}\\left(\\frac{\\left(\\tau_0 \\theta_0+\\tau\\sum_{i=1}^{n}x_{i}\\right)}{(\\tau_0+n\\tau)}, \\sigma^{-2}=\\tau_0+n\\tau \\right)\\\\\n",
    "&={}  \\left[\\prod_{i=1}^n\\sqrt{\\frac{\\tau}{2\\pi}} e^{-\\frac{\\tau\\left(x_i-\\theta\\right)^2}{2}}\\right] \\sqrt{\\frac{\\tau_0}{2\\pi}} e^{-\\frac{\\tau_0\\left(\\theta-\\theta_0\\right)^2}{2}} \\frac{\\beta ^{\\alpha}}{\\Gamma(\\alpha)} \\tau^{\\alpha -1}e^{-\\beta \\tau} & p(\\tau|x, \\alpha, \\beta, \\theta) &={} \\text{Gamma}\\left(\\frac{\\alpha}{2}+\\frac{n}{2}, \\frac{\\beta}{2}+\\frac{1}{2}\\sum_{i=1}^n\\left(x_i-\\theta\\right)^2 \\right)\\\\{}\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "How about instead we consider a \"[location-scale-t](https://en.wikipedia.org/wiki/Student%27s_t-distribution#Location-scale_t-distribution)-norm-halfnorm-discrete-uniform\" specification?\n",
    "\n",
    "$$\\large\n",
    "\\overset{x_i\\; \\sim\\; \\text{location-scale-t}(\\mu, \\sigma^2, \\nu)}{\\quad\\quad\\quad p(x|\\mu,\\sigma^2, \\nu)} = {\\prod_{i=1}^n\n",
    "\\frac{\\Gamma\\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma\\left(\\frac{\\nu}{2}\\right) \\sqrt{\\pi \\nu \\sigma^2}}\\left(1+\\frac{1}{\\nu} \\frac{(x_i-\\mu)^2}{\\sigma^2}\\right)^{-(\\nu+1) / 2}}$$\n",
    "\n",
    "$$\\scriptsize \n",
    "\\begin{align}\n",
    "p(\\mu | \\mu_0, \\tau_0) &={} \\sqrt{\\frac{\\tau_0}{2\\pi}} e^{-\\frac{\\tau_0}{2}\\left(\\mu-\\mu_0\\right)^2} & p(\\sigma^2 | \\sigma_0^2) &={} \\sqrt{\\frac{2}{\\pi\\sigma_0^2}} \\exp \\left(-\\frac{(\\sigma^2)^2}{2 \\sigma_0^2}\\right) 1_{[0,\\infty]}(\\sigma^2) & p(\\nu=i) &={} \\Bigg\\{ \\begin{array}{cl} \\frac{1}{100} & \\text{for }i=1,\\cdots,100\\\\ 0 & \\text{otherwise} \\end{array}\\\\\n",
    "& \\textrm{normal} && \\textrm{half-normal} && \\textrm{discrete uniform}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Um yeah we're gonna need a Metroposlis cleanup on aisles one two and three  \n",
    "(or a slice or adapative squeeze rejection sampling steps... in place of Metroposlis steps)\n",
    "\n",
    "*Implement the a Metroposlis within Gibbs algorithm to smaple from the posterior of the above specification. Use a \"smallish\" sample size, say $n=100$ and implement your acceptance probability on a log-scale as described in [piazza post @65_f1](https://piazza.com/class/m5jvyco84083fm/post/65_f1)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bae1a4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import gammaln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93139e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the data\n",
    "np.random.seed(42)\n",
    "\n",
    "n = 100  # Sample size\n",
    "\n",
    "# True parameters\n",
    "true_mu = 0.0\n",
    "true_sigma2 = 1.0\n",
    "true_nu = 5\n",
    "\n",
    "# Simulate using the standard Student's t (with true_nu) and then scale & shift:\n",
    "x = true_mu + np.sqrt(true_sigma2) * stats.t.rvs(df=true_nu, size=n)\n",
    "\n",
    "\n",
    "# Log-likelihood for the location-scale t distribution\n",
    "def log_likelihood(mu, sigma2, nu, data):\n",
    "    n = len(data)\n",
    "    term1 = gammaln((nu+1)/2) - gammaln(nu/2)\n",
    "    term2 = -0.5*np.log(np.pi * nu * sigma2)\n",
    "    ll = 0.0\n",
    "    for xi in data:\n",
    "        # Using the density: (1 + (xi-mu)^2/(nu*sigma2))^(-(nu+1)/2)\n",
    "        ll += term1 + term2 - ((nu+1)/2) * np.log(1 + (xi-mu)**2/(nu*sigma2))\n",
    "    return ll\n",
    "\n",
    "# Log prior for mu ~ N(mu0, 1/tau0)\n",
    "def log_prior_mu(mu, mu0, tau0):\n",
    "    return 0.5 * np.log(tau0/(2*np.pi)) - 0.5 * tau0 * (mu - mu0)**2\n",
    "\n",
    "# Log prior for sigma^2 ~ half-normal (on sigma^2) with scale sigma0\n",
    "def log_prior_sigma2(sigma2, sigma0):\n",
    "    # density is sqrt(2/(pi*sigma0^2))*exp[-(sigma2^2)/(2*sigma0^2)] for sigma2 > 0.\n",
    "    if sigma2 <= 0:\n",
    "        return -np.inf\n",
    "    return 0.5 * np.log(2/(np.pi * sigma0**2)) - (sigma2**2)/(2*sigma0**2)\n",
    "\n",
    "# Log prior for nu ~ discrete uniform on {1,...,100}\n",
    "def log_prior_nu(nu):\n",
    "    if nu < 1 or nu > 100:\n",
    "        return -np.inf\n",
    "    return -np.log(100)\n",
    "\n",
    "# Full log-posterior (up to a constant)\n",
    "def log_posterior(mu, sigma2, nu, data, mu0, tau0, sigma0):\n",
    "    return (log_likelihood(mu, sigma2, nu, data) +\n",
    "            log_prior_mu(mu, mu0, tau0) +\n",
    "            log_prior_sigma2(sigma2, sigma0) +\n",
    "            log_prior_nu(nu))\n",
    "\n",
    "\n",
    "\n",
    "# Initialize MCMC parameters\n",
    "num_iter = 8000  # Number of MCMC samples\n",
    "burn_in = 2000\n",
    "\n",
    "mu_samples = np.zeros(num_iter)\n",
    "sigma2_samples = np.zeros(num_iter)\n",
    "nu_samples = np.zeros(num_iter, dtype=int)\n",
    "\n",
    "# Hyperparameters for the priors:\n",
    "mu0 = 0.0\n",
    "tau0 = 1.0\n",
    "sigma0 = 1.0  # scale for the half-normal on sigma^2\n",
    "\n",
    "# Metropolis proposal standard deviations\n",
    "s_mu = 0.3          # for mu update \n",
    "s_log_sigma2 = 0.3  # for log(sigma2) update\n",
    "\n",
    "# Storage for samples\n",
    "mu_samples = np.zeros(num_iter)\n",
    "sigma2_samples = np.zeros(num_iter)\n",
    "nu_samples = np.zeros(num_iter, dtype=int)\n",
    "\n",
    "# Initialize parameters\n",
    "mu_current = np.mean(x)\n",
    "sigma2_current = np.var(x)\n",
    "nu_current = 10   # start with a moderate nu\n",
    "\n",
    "current_log_post = log_posterior(mu_current, sigma2_current, nu_current, x, mu0, tau0, sigma0)\n",
    "\n",
    "\n",
    "# MCMC sampler (Gibbs-with-MH for continuous params and direct sampling for nu)\n",
    "for it in range(num_iter):\n",
    "    # Update mu using MH\n",
    "    mu_prop = stats.norm.rvs(loc=mu_current, scale=s_mu)\n",
    "    log_post_prop = log_posterior(mu_prop, sigma2_current, nu_current, x, mu0, tau0, sigma0)\n",
    "    if np.log(np.random.rand()) < (log_post_prop - current_log_post):\n",
    "        mu_current = mu_prop\n",
    "        current_log_post = log_post_prop\n",
    "\n",
    "    # Update sigma^2 using MH on the log scale \n",
    "    log_sigma2_current = np.log(sigma2_current)\n",
    "    log_sigma2_prop = stats.norm.rvs(loc=log_sigma2_current, scale=s_log_sigma2)\n",
    "    sigma2_prop = np.exp(log_sigma2_prop)\n",
    "    log_post_prop = log_posterior(mu_current, sigma2_prop, nu_current, x, mu0, tau0, sigma0)\n",
    "    \n",
    "    # include Jacobian adjustment: proposal density ratio = sigma2_prop / sigma2_current\n",
    "    log_accept_ratio = log_post_prop - current_log_post + np.log(sigma2_prop) - np.log(sigma2_current)\n",
    "    if np.log(np.random.rand()) < log_accept_ratio:\n",
    "        sigma2_current = sigma2_prop\n",
    "        current_log_post = log_post_prop\n",
    "\n",
    "    # Update nu (discrete), the (unnormalized) log probability for each candidate nu in {1,...,100}\n",
    "    log_probs = np.array([log_likelihood(mu_current, sigma2_current, nu_candidate, x) +\n",
    "                            log_prior_nu(nu_candidate)\n",
    "                            for nu_candidate in range(1, 101)])\n",
    "    \n",
    "    # To avoid numerical issues, subtract the maximum log probability\n",
    "    log_probs -= np.max(log_probs)\n",
    "    probs = np.exp(log_probs)\n",
    "    probs /= probs.sum()\n",
    "    # Sample a new nu from 1,...,100\n",
    "    nu_current = np.random.choice(np.arange(1, 101), p=probs)\n",
    "    # Update the current log posterior to reflect the new nu:\n",
    "    current_log_post = log_posterior(mu_current, sigma2_current, nu_current, x, mu0, tau0, sigma0)\n",
    "\n",
    "    # Store samples\n",
    "    mu_samples[it] = mu_current\n",
    "    sigma2_samples[it] = sigma2_current\n",
    "    nu_samples[it] = nu_current\n",
    "\n",
    "# Discard burn-in\n",
    "mu_samples = mu_samples[burn_in:]\n",
    "sigma2_samples = sigma2_samples[burn_in:]\n",
    "nu_samples = nu_samples[burn_in:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393faebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Plotting the results -----\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Trace plot for mu\n",
    "plt.subplot(3, 2, 1)\n",
    "plt.plot(mu_samples, color='blue')\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"mu\")\n",
    "plt.title(\"Trace Plot for mu\")\n",
    "\n",
    "# Histogram for mu\n",
    "plt.subplot(3, 2, 2)\n",
    "plt.hist(mu_samples, bins=30, density=True, color='blue', alpha=0.7)\n",
    "plt.xlabel(\"mu\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Posterior of mu\")\n",
    "\n",
    "# Trace plot for sigma^2\n",
    "plt.subplot(3, 2, 3)\n",
    "plt.plot(sigma2_samples, color='green')\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"sigma^2\")\n",
    "plt.title(\"Trace Plot for sigma^2\")\n",
    "\n",
    "# Histogram for sigma^2\n",
    "plt.subplot(3, 2, 4)\n",
    "plt.hist(sigma2_samples, bins=30, density=True, color='green', alpha=0.7)\n",
    "plt.xlabel(\"sigma^2\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Posterior of sigma^2\")\n",
    "\n",
    "# Trace plot for nu (discrete parameter)\n",
    "plt.subplot(3, 2, 5)\n",
    "plt.plot(nu_samples, color='red')\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"nu\")\n",
    "plt.title(\"Trace Plot for nu\")\n",
    "\n",
    "# Histogram for nu\n",
    "plt.subplot(3, 2, 6)\n",
    "plt.hist(nu_samples, color='red', alpha=0.7)\n",
    "plt.xlabel(\"nu\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Posterior of nu\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0d98a3",
   "metadata": {},
   "source": [
    "### Q2: explore the role of sample size in providing inference for the degrees of freedom parameter $\\nu$\n",
    "\n",
    "*Implement the specification above using `PyMC` where you can explore inference on $\\nu$ at different sample sizes. Provide a summarization and explanation of your findings.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6171772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5db6ec8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (2 chains in 4 jobs)\n",
      "CompoundStep\n",
      ">NUTS: [mu, sigma]\n",
      ">Metropolis: [nu]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='6000' class='' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [6000/6000 00:03&lt;00:00 Sampling 2 chains, 0 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 2 chains for 1_000 tune and 2_000 draw iterations (2_000 + 4_000 draws total) took 4 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n",
      "Multiprocess sampling (2 chains in 4 jobs)\n",
      "CompoundStep\n",
      ">NUTS: [mu, sigma]\n",
      ">Metropolis: [nu]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='6000' class='' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [6000/6000 00:03&lt;00:00 Sampling 2 chains, 0 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 2 chains for 1_000 tune and 2_000 draw iterations (2_000 + 4_000 draws total) took 4 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n",
      "Multiprocess sampling (2 chains in 4 jobs)\n",
      "CompoundStep\n",
      ">NUTS: [mu, sigma]\n",
      ">Metropolis: [nu]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='6000' class='' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [6000/6000 00:04&lt;00:00 Sampling 2 chains, 0 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 2 chains for 1_000 tune and 2_000 draw iterations (2_000 + 4_000 draws total) took 4 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n",
      "Multiprocess sampling (2 chains in 4 jobs)\n",
      "CompoundStep\n",
      ">NUTS: [mu, sigma]\n",
      ">Metropolis: [nu]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='6000' class='' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [6000/6000 00:04&lt;00:00 Sampling 2 chains, 0 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 2 chains for 1_000 tune and 2_000 draw iterations (2_000 + 4_000 draws total) took 4 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{50:       mean      sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  ess_bulk  ess_tail  \\\n",
       " nu  56.057  26.924    14.0    100.0      0.883    0.624     945.0    1088.0   \n",
       " \n",
       "     r_hat  \n",
       " nu    1.0  ,\n",
       " 100:      mean    sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  ess_bulk  ess_tail  \\\n",
       " nu  2.242  0.62     1.0      3.0      0.045    0.032     173.0     125.0   \n",
       " \n",
       "     r_hat  \n",
       " nu   1.03  ,\n",
       " 500:      mean     sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  ess_bulk  ess_tail  \\\n",
       " nu  7.952  5.139     4.0     13.0      0.422    0.299     227.0     248.0   \n",
       " \n",
       "     r_hat  \n",
       " nu   1.01  ,\n",
       " 900:     mean     sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  ess_bulk  ess_tail  \\\n",
       " nu  7.03  1.691     5.0     10.0      0.094    0.066     269.0     251.0   \n",
       " \n",
       "     r_hat  \n",
       " nu   1.02  }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Different sample sizes to test\n",
    "sample_sizes = [50, 100, 500, 900]\n",
    "\n",
    "# True parameters\n",
    "true_mu = 0.0\n",
    "true_sigma = 1.0\n",
    "true_nu = 5\n",
    "\n",
    "# Store inference results\n",
    "results = {}\n",
    "\n",
    "for n in sample_sizes:\n",
    "    # Generate synthetic data from a Student's t-distribution\n",
    "    x = true_mu + true_sigma * stats.t.rvs(df=true_nu, size=n)\n",
    "    \n",
    "    # Define PyMC model\n",
    "    with pm.Model() as model:\n",
    "        mu = pm.Normal(\"mu\", mu=0, sigma=10)\n",
    "        sigma = pm.HalfNormal(\"sigma\", sigma=10)\n",
    "        nu = pm.DiscreteUniform(\"nu\", lower=1, upper=100)\n",
    "        \n",
    "        x_obs = pm.StudentT(\"x_obs\", nu=nu, mu=mu, sigma=sigma, observed=x)\n",
    "        \n",
    "        # Inference\n",
    "        trace = pm.sample(2000, tune=1000, chains=2, return_inferencedata=True, target_accept=0.9)\n",
    "    \n",
    "    results[n] = trace\n",
    "\n",
    "# Summarize results\n",
    "display({n: az.summary(results[n], var_names=[\"nu\"]) for n in sample_sizes})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6983f6a6",
   "metadata": {},
   "source": [
    "We can see that small sample sizes with 50, posterior distribution for $v$ is quite dispersed, indicating that the data do not contain enough information to precisely determine the tail thickness. $v's$ posterior may fluctuate over a wide range, reflecting considerable uncertainty.\n",
    "\n",
    "Moderate sample sizes with 100 and 500, as the sample size increases, the posterior distribution starts to concentrate within a narrower interval. Compared to a small sample, the data provide more information, leading to more accurate inference on $v$.\n",
    "\n",
    "The posterior distribution of larger sample sizes over 500 becomes even more concentrated, clearly clustering around the true value ($v=5$ in this case). This demonstrates that when the sample size is sufficiently large, the data can significantly reduce the uncertainty in $v$, yielding a more precise estimate of the tail thickness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306b3612",
   "metadata": {},
   "source": [
    "### Q3: the posterior predictive distribution does something like the following...\n",
    "\n",
    "Let $p(\\tau_i)$ be $\\require{cancel} \\textrm{gamma}\\big(\\tau_i | \\alpha = \\frac{\\nu}{2}, \\overset{\\textrm{rate}\\xcancel{\\textrm{scale}}}{\\beta = \\frac{\\nu}{2}}\\big)$ and let $p(y_i|\\tau_i)$ be $\\textrm{N}(y_i | 0,\\tau_i)$ and now integrate out the uncertainty in $\\tau_i$ and see what distribution is left over for $y_i$.\n",
    "\n",
    "*Go look at the gamma distribution and remember that you know that the integrals of unnormalized densities are the inverse of their normalizing constants. Then go look at the t distribution and determine what distribution the following expression defines. Then explain why the behavior demonstrated here is analagous to that of the posterior predictive distribution.*\n",
    "\n",
    "$$\\int p(y_i|\\tau_i) p(\\tau_i)  d\\tau_i = \\int \\sqrt{\\frac{\\tau_i}{2\\pi}}e^{-\\frac{1}{2}\\tau_i y_i^2} \\frac {\\frac{\\nu}{2}^{\\frac{\\nu}{2}}}{\\Gamma \\left(\\frac{\\nu}{2}\\right)} \\tau_i^{\\frac{\\nu}{2}-1}e^{-\\frac{\\nu}{2}\\tau_i} d\\tau_i$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a77c2a9",
   "metadata": {},
   "source": [
    "#### Q3\n",
    "We need to derive the posterior predictive distribution by integrating out the uncertainty in ( $\\tau_i$):\n",
    "\n",
    " $$\n",
    "p(y_i) = \\int p(y_i | \\tau_i) p(\\tau_i) d\\tau_i\n",
    " $$\n",
    "\n",
    "#### **Step 1: Define the Given Distributions**\n",
    "We have:\n",
    "\n",
    "- p($\\tau_i$) follows a Gamma distribution:\n",
    "  \n",
    "  $$\n",
    "  \\tau_i \\sim \\text{Gamma} \\left( \\frac{\\nu}{2}, \\frac{\\nu}{2} \\right)\n",
    "  $$\n",
    "\n",
    "  This means its probability density function (PDF) is:\n",
    "\n",
    "  $$\n",
    "  p(\\tau_i) = \\frac{\\left(\\frac{\\nu}{2}\\right)^{\\nu/2}}{\\Gamma(\\nu/2)} \\tau_i^{\\nu/2 - 1} e^{-\\frac{\\nu}{2} \\tau_i}\n",
    "  $$\n",
    "\n",
    "\n",
    "- p($y_i | \\tau_i$)  follows a Normal distribution:\n",
    "\n",
    "  $$\n",
    "  y_i | \\tau_i \\sim N(0, \\tau_i^{-1})\n",
    "  $$\n",
    "\n",
    "\n",
    "- Thus, the likelihood is: \n",
    "\n",
    "  $$\n",
    "  p(y_i | \\tau_i) = \\sqrt{\\frac{\\tau_i}{2\\pi}} e^{-\\frac{1}{2} \\tau_i y_i^2}\n",
    "  $$\n",
    "\n",
    "#### **Step 2: Compute the Integral**\n",
    "We need to evaluate:\n",
    "\n",
    "$$\n",
    "p(y_i) = \\int p(y_i | \\tau_i) p(\\tau_i) d\\tau_i\n",
    "$$\n",
    "\n",
    "Substituting the given expressions:\n",
    "\n",
    "$$\n",
    "p(y_i) = \\int \\left( \\sqrt{\\frac{\\tau_i}{2\\pi}} e^{-\\frac{1}{2} \\tau_i y_i^2} \\right) \\cdot \\left( \\frac{\\left(\\frac{\\nu}{2}\\right)^{\\nu/2}}{\\Gamma(\\nu/2)} \\tau_i^{\\nu/2 - 1} e^{-\\frac{\\nu}{2} \\tau_i} \\right) d\\tau_i\n",
    "$$\n",
    "\n",
    "Rearrange the terms:\n",
    "\n",
    "$$\n",
    "p(y_i) = \\frac{\\left(\\frac{\\nu}{2}\\right)^{\\nu/2}}{\\Gamma(\\nu/2)} \\sqrt{\\frac{1}{2\\pi}} \\int \\tau_i^{\\nu/2 - 1 + 1/2} e^{-\\frac{1}{2}(\\nu + y_i^2) \\tau_i} d\\tau_i\n",
    "$$\n",
    "\n",
    "Combining the powers of ( $\\tau_i$):\n",
    "\n",
    "$$\n",
    "\\tau_i^{\\nu/2 - 1 + 1/2} = \\tau_i^{\\frac{\\nu - 1}{2}}\n",
    "$$\n",
    "\n",
    "Now the integral is of the form:\n",
    "\n",
    "$$\n",
    "\\int x^{a-1} e^{-bx} dx = \\frac{\\Gamma(a)}{b^a}\n",
    "$$\n",
    "\n",
    "where:\n",
    "$$\n",
    "a = \\frac{\\nu + 1}{2},\n",
    "b = \\frac{\\nu + y_i^2}{2}\n",
    "$$\n",
    "\n",
    "Applying this identity:\n",
    "\n",
    "$$\n",
    "p(y_i) = \\frac{\\left(\\frac{\\nu}{2}\\right)^{\\nu/2}}{\\Gamma(\\nu/2)} \\sqrt{\\frac{1}{2\\pi}} \\cdot \\frac{\\Gamma(\\frac{\\nu+1}{2})}{\\left( \\frac{\\nu + y_i^2}{2} \\right)^{(\\nu+1)/2}}\n",
    "$$\n",
    "\n",
    "Using the Gamma function property:\n",
    "\n",
    "$$\n",
    "\\frac{\\Gamma(x + \\frac{1}{2})}{\\Gamma(x) \\sqrt{x}} = \\frac{\\Gamma(\\frac{\\nu+1}{2})}{\\Gamma(\\nu/2)} \\cdot \\sqrt{\\frac{\\nu}{\\pi}}\n",
    "$$\n",
    "\n",
    "we simplify:\n",
    "\n",
    "$$\n",
    "p(y_i) = \\frac{\\Gamma(\\frac{\\nu+1}{2})}{\\Gamma(\\nu/2) \\sqrt{\\pi \\nu}} \\left( 1 + \\frac{y_i^2}{\\nu} \\right)^{-\\frac{\\nu+1}{2}}\n",
    "$$\n",
    "\n",
    "#### **Step 3: Recognizing the Result**\n",
    "This matches the probability density function of the **Student’s t-distribution**:\n",
    "\n",
    "$$\n",
    "y_i \\sim t_{\\nu}\n",
    "$$\n",
    "\n",
    "Thus, integrating out ( $\\tau_i$) results in a Student’s t-distribution with ( $\\nu$ ) degrees of freedom, location 0 , and scale 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee45fc72",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Interpretation**\n",
    "Even though we originally modeled ( $y_i$ ) as conditionally Normal given ($\\tau_i$), the marginal distribution (after integrating out ( $\\tau_i$ ) follows a Student’s t-distribution. The Gamma prior on $\\tau_i$ acts as an inverse-variance prior, introducing extra uncertainty and making the tails heavier. This explains why the posterior predictive distribution behaves like a t-distribution, making it robust to outliers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
